{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ckz2011/ClothMatchMaker/blob/main/Clothing_Match_Maker_Assistant_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a979af8",
      "metadata": {
        "id": "5a979af8"
      },
      "source": [
        "# 🧥 Clothing Match Maker Assistant using RAG + GPT-4 Vision\n",
        "\n",
        "This notebook demonstrates a Clothing Recommendation system using image analysis and RAG with GPT-4 Vision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "7cee3440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7cee3440",
        "outputId": "c0e4ce7e-00c5-4ec5-82ed-2cbad419f0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m61.4/70.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community langchain-openai langchain openai tiktoken --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513918a3",
      "metadata": {
        "id": "513918a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "!curl -O https://raw.githubusercontent.com/anshupandey/Generative-AI-for-Professionals/main/datasets/sample_clothes.zip\n",
        "!unzip -o sample_clothes.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "8fee1350",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fee1350",
        "outputId": "98ccdc12-53c4-437f-fea0-0ccdf4186466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Azure OpenAI Endpoint: https://eastus2.api.cognitive.microsoft.com/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import ast\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from IPython.display import Image, display, HTML\n",
        "import concurrent.futures\n",
        "import tiktoken\n",
        "from google.colab import userdata, files\n",
        "\n",
        "from langchain_community.chat_models.azure_openai import AzureChatOpenAI\n",
        "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "\n",
        "# Config\n",
        "GPT_MODEL = \"gpt-4\"\n",
        "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
        "EMBEDDING_COST_PER_1K_TOKENS = 0.00013\n",
        "curr_path = os.getcwd()\n",
        "\n",
        "\n",
        "env_variables = \"\"\"\n",
        "AZURE_OPENAI_ENDPOINT=XXXXXXXXXXXXXX\n",
        "AZURE_OPENAI_API_KEY=XXXXXXXXXXXXXX\n",
        "OPENAI_API_VERSION=XXXXXXXXXXXXXX\n",
        "\"\"\"\n",
        "\n",
        "# Create the .env file in the current directory\n",
        "with open('.env', 'w') as f:\n",
        "    f.write(env_variables)\n",
        "\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access the environment variables\n",
        "azure_openai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
        "openai_api_version = os.getenv('OPENAI_API_VERSION')\n",
        "\n",
        "\n",
        "# Print to verify\n",
        "print(f\"Azure OpenAI Endpoint: {azure_openai_endpoint}\")\n",
        "\n",
        "client = AzureChatOpenAI(\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
        "    model=GPT_MODEL,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "embedding_client = OpenAIEmbeddings(\n",
        "    model=EMBEDDING_MODEL,\n",
        "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    openai_api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5a2d8970",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2d8970",
        "outputId": "f5ffb688-2041-4e0d-9383-622a3f39430c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Dataset with 1000 entries\n"
          ]
        }
      ],
      "source": [
        "styles_filepath = os.path.join(curr_path, \"sample_clothes\", \"sample_styles.csv\")\n",
        "styles_df = pd.read_csv(styles_filepath, on_bad_lines='skip')\n",
        "print(\"Loaded Dataset with\", len(styles_df), \"entries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fa9135",
      "metadata": {
        "id": "24fa9135"
      },
      "outputs": [],
      "source": [
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(10))\n",
        "def get_embeddings(input: list):\n",
        "    try:\n",
        "        response = embedding_client.embed_documents(input)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_embeddings: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def batchify(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx : min(ndx + n, l)]\n",
        "\n",
        "def embed_corpus(corpus: List[str], batch_size=64, num_workers=8, max_context_len=8191):\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    encoded_corpus = [encoding.encode(text)[:max_context_len] for text in corpus]\n",
        "    num_tokens = sum(len(article) for article in encoded_corpus)\n",
        "    cost_to_embed_tokens = num_tokens / 1000 * EMBEDDING_COST_PER_1K_TOKENS\n",
        "    print(f\"num_articles={len(encoded_corpus)}, num_tokens={num_tokens}, est_embedding_cost={cost_to_embed_tokens:.4f} USD\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        futures = [executor.submit(get_embeddings, batch) for batch in batchify(corpus, batch_size)]\n",
        "        with tqdm(total=len(corpus)) as pbar:\n",
        "            for _ in concurrent.futures.as_completed(futures):\n",
        "                pbar.update(batch_size)\n",
        "        embeddings = []\n",
        "        for f in futures:\n",
        "            embeddings.extend(f.result())\n",
        "    return embeddings\n",
        "\n",
        "def generate_embeddings(df, column_name):\n",
        "    descriptions = df[column_name].astype(str).tolist()\n",
        "    df['embeddings'] = embed_corpus(descriptions)\n",
        "    return df\n",
        "\n",
        "styles_df = generate_embeddings(styles_df, 'productDisplayName')\n",
        "styles_df.to_csv('sample_clothes/sample_styles_with_embeddings.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c9a61c",
      "metadata": {
        "id": "26c9a61c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cosine_similarity_manual(vec1, vec2):\n",
        "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
        "    dot = np.dot(vec1, vec2)\n",
        "    norm1, norm2 = np.linalg.norm(vec1), np.linalg.norm(vec2)\n",
        "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
        "\n",
        "def find_similar_items(input_embedding, embeddings, threshold=0.5, top_k=2):\n",
        "    sims = [(i, cosine_similarity_manual(input_embedding, vec)) for i, vec in enumerate(embeddings)]\n",
        "    filtered = [(i, s) for i, s in sims if s >= threshold]\n",
        "    return sorted(filtered, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "def find_matching_items_with_rag(df_items, item_descs):\n",
        "    embeddings = df_items['embeddings'].tolist()\n",
        "    similar_items = []\n",
        "    for desc in item_descs:\n",
        "        input_embedding = get_embeddings([desc])[0]\n",
        "        matches = find_similar_items(input_embedding, embeddings)\n",
        "        similar_items.extend([df_items.iloc[i] for i, _ in matches])\n",
        "    return similar_items\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b90ea5",
      "metadata": {
        "id": "42b90ea5"
      },
      "outputs": [],
      "source": [
        "\n",
        "uploaded = files.upload()\n",
        "uploaded_path = list(uploaded.keys())[0]\n",
        "\n",
        "def encode_image_to_base64(image_path):\n",
        "    with open(image_path, 'rb') as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "encoded_image = encode_image_to_base64(uploaded_path)\n",
        "\n",
        "def analyze_image(image_base64, subcategories):\n",
        "    messages = [\n",
        "        HumanMessage(content=[\n",
        "            {\"type\": \"text\", \"text\": f\"\"\"Given an image of an item of clothing, return JSON with: 'items', 'category', 'gender'.\n",
        "            Choose category from: {subcategories}. Gender from: [Men, Women, Boys, Girls, Unisex].\n",
        "            Example Input: A black leather jacket.\n",
        "            Example Output: {{\"items\": [\"White Tee\", \"Skinny Jeans\"], \"category\": \"Jackets\", \"gender\": \"Women\"}}\"\"\"},\n",
        "            {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
        "        ])\n",
        "    ]\n",
        "    response = client.invoke(messages)\n",
        "    return response.content\n",
        "\n",
        "unique_subcategories = styles_df['articleType'].unique()\n",
        "analysis = json.loads(analyze_image(encoded_image, unique_subcategories))\n",
        "\n",
        "item_descs = analysis['items']\n",
        "item_category = analysis['category']\n",
        "item_gender = analysis['gender']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9fd77c3",
      "metadata": {
        "id": "a9fd77c3"
      },
      "outputs": [],
      "source": [
        "\n",
        "filtered_items = styles_df[(styles_df['gender'].isin([item_gender, 'Unisex'])) & (styles_df['articleType'] != item_category)]\n",
        "matching_items = find_matching_items_with_rag(filtered_items, item_descs)\n",
        "\n",
        "html = \"<h3>Uploaded Image:</h3>\"\n",
        "display(Image(filename=uploaded_path))\n",
        "\n",
        "html += \"<h3>Matching Items:</h3><div style='display:flex;flex-wrap:wrap'>\"\n",
        "paths = []\n",
        "image_folder = os.path.join(curr_path, \"sample_clothes\", \"sample_clothes\", \"sample_images\")\n",
        "for item in matching_items:\n",
        "    item_id = item['id']\n",
        "    item_path = os.path.join(image_folder, f\"{item_id}.jpg\")\n",
        "    paths.append(item_path)\n",
        "    display(Image(filename=item_path))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}